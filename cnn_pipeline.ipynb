{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8Z8aQd8RTf_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/DATA/VHT-DATA/data.zip /content/data.zip\n",
        "\n",
        "!unzip /content/data.zip\n"
      ],
      "metadata": {
        "id": "xtTJUOS9ToWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "id": "frAaRm7hp4gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def resize_image(input_path, output_path, size=(512, 640)):\n",
        "    \"\"\"\n",
        "    Resize the image to the specified size.\n",
        "\n",
        "    :param input_path: Path to the input image.\n",
        "    :param output_path: Path to save the resized image.\n",
        "    :param size: Desired size as a tuple (width, height).\n",
        "    \"\"\"\n",
        "    with Image.open(input_path) as img:\n",
        "        inputs = ImageLoader.load_image(img)\n",
        "\n",
        "        preds = model(inputs)\n",
        "        # resized_img = img.resize(size, Image.LANCZOS)\n",
        "        # resized_img.save(output_path)\n",
        "        # image = cv2.imread(output_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # kernel = np.array([[0, -1, 0],\n",
        "        #                       [-1, 5, -1],\n",
        "        #                       [0, -1, 0]])\n",
        "        # image = cv2.filter2D(image, -1, kernel)\n",
        "\n",
        "        # image = cv2.bilateralFilter(image, d=1, sigmaColor=5, sigmaSpace=10)\n",
        "        # clahe = cv2.createCLAHE(clipLimit=10.0, tileGridSize=(2, 2))\n",
        "        # image = clahe.apply(image)\n",
        "        # image = cv2.GaussianBlur(image, (7, 7), 0)\n",
        "        ImageLoader.save_image(preds, output_path)\n",
        "        ImageLoader.save_compare(inputs, preds, output_path.replace(\"SD-up\", \"temp\"))\n",
        "\n",
        "        # cv2.imwrite(output_path, resized_img)\n",
        "\n",
        "def compute_ssim(original_path, resized_path):\n",
        "    \"\"\"\n",
        "    Compute SSIM between the original and resized images.\n",
        "\n",
        "    :param original_path: Path to the original image.\n",
        "    :param resized_path: Path to the resized image.\n",
        "    :return: SSIM value.\n",
        "    \"\"\"\n",
        "    original = Image.open(original_path).convert('L')\n",
        "    resized = Image.open(resized_path).convert('L')\n",
        "\n",
        "    original_np = np.array(original)\n",
        "    resized_np = np.array(resized)\n",
        "    return ssim(original_np, resized_np)\n",
        "\n",
        "def process_images(input_dir, output_dir, size=(1280, 1024)):\n",
        "    \"\"\"\n",
        "    Resize all images in the input directory and compute SSIM.\n",
        "\n",
        "    :param input_dir: Directory with the original images.\n",
        "    :param output_dir: Directory to save the resized images.\n",
        "    :param size: Desired size for resizing.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    score = []\n",
        "    for filename in tqdm(os.listdir(input_dir)):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.bmp')):\n",
        "            original_path = os.path.join(input_dir, filename)\n",
        "            resized_path = os.path.join(output_dir, filename)\n",
        "            resize_image(original_path, resized_path, size)\n",
        "            root_image = original_path.replace(\"SD\",\"HD\")\n",
        "            ssim_value = compute_ssim(root_image, resized_path)\n",
        "            score.append(ssim_value)\n",
        "    print(f\"AVG: {sum(score)/len(score)}\")\n",
        "\n",
        "input_directory = '/content/data/val/SD'\n",
        "output_dir = '/content/data/val/SD-up'\n",
        "process_images(input_directory, output_dir)\n",
        "\n",
        "input_directory = '/content/data/train/SD'\n",
        "output_dir = '/content/data/train/SD-up'\n",
        "process_images(input_directory, output_dir)\n"
      ],
      "metadata": {
        "id": "YMh-YsDUZxOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "_logger = logging.getLogger(__name__)\n",
        "\n",
        "from typing import Dict, Any\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class Scheduler:\n",
        "    def __init__(self,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 param_group_field: str,\n",
        "                 noise_range_t=None,\n",
        "                 noise_type='normal',\n",
        "                 noise_pct=0.67,\n",
        "                 noise_std=1.0,\n",
        "                 noise_seed=None,\n",
        "                 initialize: bool = True) -> None:\n",
        "        self.optimizer = optimizer\n",
        "        self.param_group_field = param_group_field\n",
        "        self._initial_param_group_field = f\"initial_{param_group_field}\"\n",
        "        if initialize:\n",
        "            for i, group in enumerate(self.optimizer.param_groups):\n",
        "                if param_group_field not in group:\n",
        "                    raise KeyError(f\"{param_group_field} missing from param_groups[{i}]\")\n",
        "                group.setdefault(self._initial_param_group_field, group[param_group_field])\n",
        "        else:\n",
        "            for i, group in enumerate(self.optimizer.param_groups):\n",
        "                if self._initial_param_group_field not in group:\n",
        "                    raise KeyError(f\"{self._initial_param_group_field} missing from param_groups[{i}]\")\n",
        "        self.base_values = [group[self._initial_param_group_field] for group in self.optimizer.param_groups]\n",
        "        self.metric = None  # any point to having this for all?\n",
        "        self.noise_range_t = noise_range_t\n",
        "        self.noise_pct = noise_pct\n",
        "        self.noise_type = noise_type\n",
        "        self.noise_std = noise_std\n",
        "        self.noise_seed = noise_seed if noise_seed is not None else 42\n",
        "        self.update_groups(self.base_values)\n",
        "\n",
        "    def state_dict(self) -> Dict[str, Any]:\n",
        "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
        "\n",
        "    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n",
        "        self.__dict__.update(state_dict)\n",
        "\n",
        "    def get_epoch_values(self, epoch: int):\n",
        "        return None\n",
        "\n",
        "    def get_update_values(self, num_updates: int):\n",
        "        return None\n",
        "\n",
        "    def step(self, epoch: int, metric: float = None) -> None:\n",
        "        self.metric = metric\n",
        "        values = self.get_epoch_values(epoch)\n",
        "        if values is not None:\n",
        "            values = self._add_noise(values, epoch)\n",
        "            self.update_groups(values)\n",
        "\n",
        "    def step_update(self, num_updates: int, metric: float = None):\n",
        "        self.metric = metric\n",
        "        values = self.get_update_values(num_updates)\n",
        "        if values is not None:\n",
        "            values = self._add_noise(values, num_updates)\n",
        "            self.update_groups(values)\n",
        "\n",
        "    def update_groups(self, values):\n",
        "        if not isinstance(values, (list, tuple)):\n",
        "            values = [values] * len(self.optimizer.param_groups)\n",
        "        for param_group, value in zip(self.optimizer.param_groups, values):\n",
        "            if 'lr_scale' in param_group:\n",
        "                param_group[self.param_group_field] = value * param_group['lr_scale']\n",
        "            else:\n",
        "                param_group[self.param_group_field] = value\n",
        "\n",
        "    def _add_noise(self, lrs, t):\n",
        "        if self._is_apply_noise(t):\n",
        "            noise = self._calculate_noise(t)\n",
        "            lrs = [v + v * noise for v in lrs]\n",
        "        return lrs\n",
        "\n",
        "    def _is_apply_noise(self, t) -> bool:\n",
        "        \"\"\"Return True if scheduler in noise range.\"\"\"\n",
        "        apply_noise = False\n",
        "        if self.noise_range_t is not None:\n",
        "            if isinstance(self.noise_range_t, (list, tuple)):\n",
        "                apply_noise = self.noise_range_t[0] <= t < self.noise_range_t[1]\n",
        "            else:\n",
        "                apply_noise = t >= self.noise_range_t\n",
        "        return apply_noise\n",
        "\n",
        "    def _calculate_noise(self, t) -> float:\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(self.noise_seed + t)\n",
        "        if self.noise_type == 'normal':\n",
        "            while True:\n",
        "                # resample if noise out of percent limit, brute force but shouldn't spin much\n",
        "                noise = torch.randn(1, generator=g).item()\n",
        "                if abs(noise) < self.noise_pct:\n",
        "                    return noise\n",
        "        else:\n",
        "            noise = 2 * (torch.rand(1, generator=g).item() - 0.5) * self.noise_pct\n",
        "        return noise\n",
        "\n",
        "class CosineLRScheduler(Scheduler):\n",
        "    \"\"\"\n",
        "    Cosine decay with restarts.\n",
        "    This is described in the paper https://arxiv.org/abs/1608.03983.\n",
        "\n",
        "    Inspiration from\n",
        "    https://github.com/allenai/allennlp/blob/master/allennlp/training/learning_rate_schedulers/cosine.py\n",
        "\n",
        "    k-decay option based on `k-decay: A New Method For Learning Rate Schedule` - https://arxiv.org/abs/2004.05909\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 t_initial: int,\n",
        "                 lr_min: float = 0.,\n",
        "                 cycle_mul: float = 1.,\n",
        "                 cycle_decay: float = 1.,\n",
        "                 cycle_limit: int = 1,\n",
        "                 warmup_t=0,\n",
        "                 warmup_lr_init=0,\n",
        "                 warmup_prefix=False,\n",
        "                 t_in_epochs=True,\n",
        "                 noise_range_t=None,\n",
        "                 noise_pct=0.67,\n",
        "                 noise_std=1.0,\n",
        "                 noise_seed=42,\n",
        "                 k_decay=1.0,\n",
        "                 initialize=True) -> None:\n",
        "        super().__init__(\n",
        "            optimizer, param_group_field=\"lr\",\n",
        "            noise_range_t=noise_range_t, noise_pct=noise_pct, noise_std=noise_std, noise_seed=noise_seed,\n",
        "            initialize=initialize)\n",
        "\n",
        "        assert t_initial > 0\n",
        "        assert lr_min >= 0\n",
        "        if t_initial == 1 and cycle_mul == 1 and cycle_decay == 1:\n",
        "            _logger.warning(\"Cosine annealing scheduler will have no effect on the learning \"\n",
        "                           \"rate since t_initial = t_mul = eta_mul = 1.\")\n",
        "        self.t_initial = t_initial\n",
        "        self.lr_min = lr_min\n",
        "        self.cycle_mul = cycle_mul\n",
        "        self.cycle_decay = cycle_decay\n",
        "        self.cycle_limit = cycle_limit\n",
        "        self.warmup_t = warmup_t\n",
        "        self.warmup_lr_init = warmup_lr_init\n",
        "        self.warmup_prefix = warmup_prefix\n",
        "        self.t_in_epochs = t_in_epochs\n",
        "        self.k_decay = k_decay\n",
        "        if self.warmup_t:\n",
        "            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]\n",
        "            super().update_groups(self.warmup_lr_init)\n",
        "        else:\n",
        "            self.warmup_steps = [1 for _ in self.base_values]\n",
        "\n",
        "    def _get_lr(self, t):\n",
        "        if t < self.warmup_t:\n",
        "            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n",
        "        else:\n",
        "            if self.warmup_prefix:\n",
        "                t = t - self.warmup_t\n",
        "\n",
        "            if self.cycle_mul != 1:\n",
        "                i = math.floor(math.log(1 - t / self.t_initial * (1 - self.cycle_mul), self.cycle_mul))\n",
        "                t_i = self.cycle_mul ** i * self.t_initial\n",
        "                t_curr = t - (1 - self.cycle_mul ** i) / (1 - self.cycle_mul) * self.t_initial\n",
        "            else:\n",
        "                i = t // self.t_initial\n",
        "                t_i = self.t_initial\n",
        "                t_curr = t - (self.t_initial * i)\n",
        "\n",
        "            gamma = self.cycle_decay ** i\n",
        "            lr_max_values = [v * gamma for v in self.base_values]\n",
        "            k = self.k_decay\n",
        "\n",
        "            if i < self.cycle_limit:\n",
        "                lrs = [\n",
        "                    self.lr_min + 0.5 * (lr_max - self.lr_min) * (1 + math.cos(math.pi * t_curr ** k / t_i ** k))\n",
        "                    for lr_max in lr_max_values\n",
        "                ]\n",
        "            else:\n",
        "                lrs = [self.lr_min for _ in self.base_values]\n",
        "\n",
        "        return lrs\n",
        "\n",
        "    def get_epoch_values(self, epoch: int):\n",
        "        if self.t_in_epochs:\n",
        "            return self._get_lr(epoch)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def get_update_values(self, num_updates: int):\n",
        "        if not self.t_in_epochs:\n",
        "            return self._get_lr(num_updates)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def get_cycle_length(self, cycles=0):\n",
        "        cycles = max(1, cycles or self.cycle_limit)\n",
        "        if self.cycle_mul == 1.0:\n",
        "            return self.t_initial * cycles\n",
        "        else:\n",
        "            return int(math.floor(-self.t_initial * (self.cycle_mul ** cycles - 1) / (1 - self.cycle_mul)))\n"
      ],
      "metadata": {
        "id": "D2g68WCXy01E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDcEwPK9SP1-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from torchvision.transforms import ToPILImage\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 2\n",
        "num_epochs = 1000\n",
        "learning_rate = 3e-4\n",
        "step_size = 50\n",
        "gamma = 0.5\n",
        "crop_size=(320, 256)\n",
        "seed=42\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    \"\"\"Set random seeds.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_random_seed(seed)\n",
        "\n",
        "# Custom Dataset Class\n",
        "class SuperResolutionDataset(Dataset):\n",
        "    def __init__(self, lr_dir, hr_dir, transform=None, lr_transform=None, crop_size=crop_size):\n",
        "        self.lr_images = sorted([os.path.join(lr_dir, img) for img in os.listdir(lr_dir)])\n",
        "        self.hr_images = sorted([os.path.join(hr_dir, img) for img in os.listdir(hr_dir)])\n",
        "        self.transform = transform\n",
        "        self.lr_transform = lr_transform\n",
        "        self.size = (1280, 1024)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lr_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # output_path = self.lr_images[idx].replace(\"SD-up\", )\n",
        "        lr_image = Image.open(self.lr_images[idx]).convert(\"L\")\n",
        "        hr_image = Image.open(self.hr_images[idx]).convert(\"L\")\n",
        "        # lr_image = lr_image.resize(self.size, Image.LANCZOS)\n",
        "        # lr_image.save(output_path)\n",
        "        # lr_image = cv2.imread(output_path, cv2.IMREAD_GRAYSCALE)\n",
        "        # kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
        "        # lr_image = cv2.filter2D(lr_image, -1, kernel)\n",
        "        # lr_image = cv2.bilateralFilter(lr_image, d=1, sigmaColor=5, sigmaSpace=0)\n",
        "        # cv2.imwrite(output_path, lr_image)\n",
        "\n",
        "        # lr_image = Image.open(output_path)\n",
        "        if self.lr_transform:\n",
        "            lr_image = self.lr_transform(lr_image)\n",
        "        else:\n",
        "            lr_image = self.transform(lr_image)\n",
        "        if self.transform:\n",
        "            hr_image = self.transform(hr_image)\n",
        "\n",
        "        return lr_image, hr_image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "])\n",
        "lr_transform = transforms.Compose([\n",
        "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "])\n",
        "\n",
        "# Datasets and Dataloaders\n",
        "train_dataset = SuperResolutionDataset(lr_dir='/content/data/train/SD-up', hr_dir='/content/data/train/HD', transform=transform, lr_transform=lr_transform)\n",
        "val_dataset = SuperResolutionDataset(lr_dir='/content/data/val/SD-up', hr_dir='/content/data/val/HD', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "hEjvaFYTFO6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MyDviXkSfsCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpscaleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UpscaleModel, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # Convolutional Layer 1\n",
        "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Convolutional Layer 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Upsampling Layer\n",
        "            # nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "\n",
        "            # Convolutional Layer 3 (Output Layer)\n",
        "            nn.Conv2d(in_channels=64, out_channels=1, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "fnDxdHOazcbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, Loss, Optimizer, and Scheduler\n",
        "model = UpscaleModel().cuda()\n",
        "criterion = nn.L1Loss(reduction='mean')\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "# scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "scheduler = CosineLRScheduler(\n",
        "            optimizer,\n",
        "            t_initial=num_epochs,\n",
        "            lr_min=1e-5,\n",
        "            warmup_lr_init=0,\n",
        "            warmup_t=0,\n",
        "            k_decay= 1.0\n",
        "        )"
      ],
      "metadata": {
        "id": "e8C1IJrTo7BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for lr_images, hr_images in train_loader:\n",
        "        lr_images = lr_images.cuda()\n",
        "        hr_images = hr_images.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(lr_images)\n",
        "        # print(outputs.shape, hr_images.shape, lr_images.shape)\n",
        "        loss = criterion(outputs, hr_images)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Learning Rate Scheduler step\n",
        "\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "    # Validation Loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    total_ssim = 0.0\n",
        "    with torch.no_grad():\n",
        "        for lr_images, hr_images in val_loader:\n",
        "            lr_images = lr_images.cuda()\n",
        "            hr_images = hr_images.cuda()\n",
        "\n",
        "\n",
        "            outputs = model(lr_images)\n",
        "            loss = criterion(outputs, hr_images)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            # Calculate SSIM\n",
        "            for i in range(outputs.size(0)):\n",
        "                output_image = outputs[i].cpu().numpy().transpose(1, 2, 0)\n",
        "                hr_image_np = hr_images[i].cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "                # Ensure that the output and HR images are in the range [0, 1]\n",
        "                # output_image = (output_image - output_image.min()) / (output_image.max() - output_image.min())\n",
        "                # hr_image_np = (hr_image_np - hr_image_np.min()) / (hr_image_np.max() - hr_image_np.min())\n",
        "                total_ssim += ssim(output_image, hr_image_np, data_range=1.0, multichannel=False, channel_axis=2)\n",
        "\n",
        "    avg_ssim = total_ssim / len(val_dataset)\n",
        "    scheduler.step(epoch)\n",
        "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}, SSIM: {avg_ssim:.4f}')\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'super_resolution_model.pth')"
      ],
      "metadata": {
        "id": "sM97omgZo3VV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}